{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jjmov99/speculative_rag/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from collections import defaultdict\n",
    "from operator import add\n",
    "from pathlib import Path\n",
    "from statistics import mean\n",
    "from time import perf_counter\n",
    "from typing import Annotated, Any, Literal, TypedDict\n",
    "\n",
    "import numpy as np\n",
    "from IPython.display import Image\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.pydantic_v1 import BaseModel as LCBaseModel\n",
    "from langchain.pydantic_v1 import Field as LCField\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain_core.runnables import RunnableSequence\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.graph.state import CompiledStateGraph\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from loguru import logger\n",
    "from openai import AsyncOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from sklearn.cluster import KMeans\n",
    "from tiktoken import Encoding, encoding_for_model\n",
    "\n",
    "from qdrant_client import AsyncQdrantClient, models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Speculative RAG tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auxiliar functions (Same from `speculative_rag.ipynb` notebook.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_perspective_sampling(\n",
    "    k: int, retrieved_points: list[models.ScoredPoint], seed: int = 1399\n",
    ") -> list[list[str]]:\n",
    "    # Generate clusters\n",
    "    logger.info(\"Finding {k} clusters.\", k=k)\n",
    "    algo: Any = KMeans(n_clusters=k, random_state=seed)\n",
    "    _vectors = [point.vector for point in retrieved_points]\n",
    "    clusters: list[int] = algo.fit_predict(X=_vectors)\n",
    "\n",
    "    # Unique clusters\n",
    "    unique_clusters: set[int] = set(clusters)\n",
    "\n",
    "    # Create a dictionary with the members of each cluster\n",
    "    cluster_dict: defaultdict[int, list[int | None]] = defaultdict(list)\n",
    "    for index, cluster in enumerate(clusters):\n",
    "        cluster_dict[cluster].append(index)\n",
    "    logger.info(\"Clusters distribution: {dist}\", dist=dict(cluster_dict))\n",
    "\n",
    "    # M subsets\n",
    "    m: int = min(len(indices) for indices in cluster_dict.values())\n",
    "    logger.info(\"{m} document subsets will be created.\", m=m)\n",
    "\n",
    "    # Generate m unique subsets without replacement\n",
    "    np.random.seed(seed=seed)\n",
    "    subsets: list[list[str]] = []\n",
    "\n",
    "    for _ in range(m):\n",
    "        subset: list[int] = []\n",
    "        for cluster in unique_clusters:\n",
    "            chosen_element: int = np.random.choice(cluster_dict[cluster])\n",
    "            subset.append(chosen_element)\n",
    "            cluster_dict[cluster].remove(chosen_element)\n",
    "        subset_documents = [\n",
    "            retrieved_points[idx].payload.get(\"content\") for idx in subset\n",
    "        ]\n",
    "        subsets.append(subset_documents)\n",
    "\n",
    "    return subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_drafting_prompt: str = \"\"\"Response to the instruction. Also provide rationale for your response.\n",
    "## Instruction: {instruction}\n",
    "\n",
    "## Evidence: {evidence}\"\"\"\n",
    "\n",
    "\n",
    "class RagDraftingResponse(BaseModel):\n",
    "    rationale: str = Field(description=\"Response rationale.\")\n",
    "    response: str = Field(description=\"Response to the instruction.\")\n",
    "\n",
    "\n",
    "async def rag_drafting_generator(\n",
    "    client: AsyncOpenAI,\n",
    "    model_name: str,\n",
    "    instruction: str,\n",
    "    evidence: str,\n",
    "    **kwargs,\n",
    ") -> tuple[RagDraftingResponse, float]:\n",
    "    completion: Any = await client.beta.chat.completions.parse(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": rag_drafting_prompt.format(\n",
    "                    instruction=instruction, evidence=evidence\n",
    "                ),\n",
    "            }\n",
    "        ],\n",
    "        response_format=RagDraftingResponse,\n",
    "        temperature=0.0,\n",
    "        logprobs=True,\n",
    "        max_tokens=512,\n",
    "        **kwargs,\n",
    "    )\n",
    "    return (\n",
    "        completion.choices[0].message.parsed,\n",
    "        np.exp(mean(token.logprob for token in completion.choices[0].logprobs.content)),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_verifier_prompt: str = \"\"\"## Instruction: {instruction}\n",
    "\n",
    "## Response: {response} \n",
    "\n",
    "## Rationale: {rationale}\n",
    "\n",
    "Is the rationale good enough to support the answer? (Yes or No)\"\"\"\n",
    "\n",
    "\n",
    "async def rag_verifier_generator(\n",
    "    client: AsyncOpenAI,\n",
    "    model_name: str,\n",
    "    instruction: str,\n",
    "    evidence: str,\n",
    "    response: str,\n",
    "    rationale: str,\n",
    "    **kwargs,\n",
    ") -> tuple[Any, float]:\n",
    "    encoder: Encoding = encoding_for_model(model_name=model_name)\n",
    "    completion: Any = await client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": rag_verifier_prompt.format(\n",
    "                    instruction=instruction,\n",
    "                    evidence=evidence,\n",
    "                    response=response,\n",
    "                    rationale=rationale,\n",
    "                ),\n",
    "            }\n",
    "        ],\n",
    "        temperature=0.0,\n",
    "        logprobs=True,\n",
    "        max_tokens=2,\n",
    "        **kwargs,\n",
    "    )\n",
    "    response: str = completion.choices[0].message.content\n",
    "    cond: bool = encoder.encode(text=response.lower()) == encoder.encode(text=\"yes\")\n",
    "    p_yes: float = (\n",
    "        np.exp(mean(token.logprob for token in completion.choices[0].logprobs.content))\n",
    "        if cond\n",
    "        else 0.0\n",
    "    )  # Naive\n",
    "\n",
    "    return (response, p_yes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tool definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input for the tool\n",
    "class GetRelevantInformationInput(LCBaseModel):\n",
    "    \"\"\"The input for `get_relevant_information` tool.\"\"\"\n",
    "\n",
    "    query: str = LCField(\n",
    "        description=\"Optimized version of the user query for retrieval augmented generation (RAG). Used to find and retrieve relevant documents to inform the response to the user's original question.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool(\"get_relevant_information\", args_schema=GetRelevantInformationInput)\n",
    "async def speculative_rag(\n",
    "    query: str,\n",
    "    embedding_model: str = \"text-embedding-3-small\",\n",
    "    collection_name: str = \"speculative_rag\",\n",
    "    k: int = 2,\n",
    "    seed: int = 1399,\n",
    "    m_drafter: str = \"gpt-4o-mini-2024-07-18\",\n",
    "    m_verifier: str = \"gpt-4o-2024-08-06\",\n",
    ") -> str:\n",
    "    \"\"\"Retrieves relevant information from an vector database that contains information about ArXiv papers.\"\"\"\n",
    "    _start = perf_counter()\n",
    "\n",
    "    # Clients\n",
    "    path: Path = Path(\"qdrant_client\")\n",
    "    qdrant_client: AsyncQdrantClient = AsyncQdrantClient(path=path)\n",
    "    client: AsyncOpenAI = AsyncOpenAI()\n",
    "\n",
    "    # Generate query vector embedding\n",
    "    logger.info(\"Generating query vector...\")\n",
    "    _now: float = perf_counter()\n",
    "    query_vector: Any = await client.embeddings.create(\n",
    "        input=query, model=embedding_model\n",
    "    )\n",
    "    query_vector: list[float] = query_vector.data[0].embedding\n",
    "    logger.info(\"Query vector generated in {s:.4f} seconds.\", s=perf_counter() - _now)\n",
    "\n",
    "    # Fetching relevant documents\n",
    "    logger.info(\"Fetching relevant documents...\")\n",
    "    _now: float = perf_counter()\n",
    "    out: list[models.ScoredPoint] = await qdrant_client.search(\n",
    "        collection_name=collection_name, query_vector=query_vector, with_vectors=True\n",
    "    )\n",
    "    logger.info(\"Documents retrieved in {s:.4f} seconds.\", s=perf_counter() - _now)\n",
    "\n",
    "    # Multi Perspective Sampling\n",
    "    logger.info(\"Doing Multi Perspective Sampling...\")\n",
    "    _now: float = perf_counter()\n",
    "    sampled_docs: list[list[str]] = multi_perspective_sampling(\n",
    "        k=k, retrieved_points=out, seed=seed\n",
    "    )\n",
    "    logger.info(\n",
    "        \"Multi Perspective Sampling done in {s:.4f} seconds.\", s=perf_counter() - _now\n",
    "    )\n",
    "\n",
    "    # RAG Drafting\n",
    "    logger.info(\"Doing RAG Drafting...\")\n",
    "    _now: float = perf_counter()\n",
    "    rag_drafts: list[tuple[RagDraftingResponse, float]] = await asyncio.gather(\n",
    "        *[\n",
    "            rag_drafting_generator(\n",
    "                client=client,\n",
    "                model_name=m_drafter,\n",
    "                instruction=query,\n",
    "                evidence=\"\\n\".join(\n",
    "                    [f\"[{idx}] {doc}\" for idx, doc in enumerate(subset, start=1)]\n",
    "                ),\n",
    "            )\n",
    "            for subset in sampled_docs\n",
    "        ]\n",
    "    )\n",
    "    logger.info(\"RAG Drafting done in {s:.4f} seconds.\", s=perf_counter() - _now)\n",
    "\n",
    "    # RAG Verifier\n",
    "    logger.info(\"Doing RAG Verification...\")\n",
    "    _now: float = perf_counter()\n",
    "    rag_verifications: list[tuple[str, float]] = await asyncio.gather(\n",
    "        *[\n",
    "            rag_verifier_generator(\n",
    "                client=client,\n",
    "                model_name=m_verifier,\n",
    "                instruction=query,\n",
    "                evidence=\"\\n\".join(\n",
    "                    [f\"[{idx}] {doc}\" for idx, doc in enumerate(subset, start=1)]\n",
    "                ),\n",
    "                response=rag_drafting_response.response,\n",
    "                rationale=rag_drafting_response.rationale,\n",
    "            )\n",
    "            for subset, (rag_drafting_response, _) in zip(sampled_docs, rag_drafts)\n",
    "        ]\n",
    "    )\n",
    "    logger.info(\"RAG Verification done in {s:.4f} seconds.\", s=perf_counter() - _now)\n",
    "\n",
    "    best_answer: int = np.argmax(\n",
    "        p_draft * p_self\n",
    "        for (_, p_draft), (_, p_self) in zip(rag_drafts, rag_verifications)\n",
    "    )\n",
    "    logger.info(\"Entire process done in {s:.4f} seconds.\", s=perf_counter() - _start)\n",
    "    return rag_drafts[best_answer][0].response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-27 21:41:59.387\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mspeculative_rag\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mGenerating query vector...\u001b[0m\n",
      "\u001b[32m2024-08-27 21:41:59.746\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mspeculative_rag\u001b[0m:\u001b[36m26\u001b[0m - \u001b[1mQuery vector generated in 0.3591 seconds.\u001b[0m\n",
      "\u001b[32m2024-08-27 21:41:59.747\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mspeculative_rag\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mFetching relevant documents...\u001b[0m\n",
      "\u001b[32m2024-08-27 21:41:59.751\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mspeculative_rag\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1mDocuments retrieved in 0.0038 seconds.\u001b[0m\n",
      "\u001b[32m2024-08-27 21:41:59.753\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mspeculative_rag\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mDoing Multi Perspective Sampling...\u001b[0m\n",
      "\u001b[32m2024-08-27 21:41:59.753\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmulti_perspective_sampling\u001b[0m:\u001b[36m5\u001b[0m - \u001b[1mFinding 2 clusters.\u001b[0m\n",
      "\u001b[32m2024-08-27 21:41:59.812\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmulti_perspective_sampling\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mClusters distribution: {1: [0, 2, 3, 4, 7, 8], 0: [1, 5, 6, 9]}\u001b[0m\n",
      "\u001b[32m2024-08-27 21:41:59.812\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmulti_perspective_sampling\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1m4 document subsets will be created.\u001b[0m\n",
      "\u001b[32m2024-08-27 21:41:59.813\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mspeculative_rag\u001b[0m:\u001b[36m42\u001b[0m - \u001b[1mMulti Perspective Sampling done in 0.0597 seconds.\u001b[0m\n",
      "\u001b[32m2024-08-27 21:41:59.813\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mspeculative_rag\u001b[0m:\u001b[36m47\u001b[0m - \u001b[1mDoing RAG Drafting...\u001b[0m\n",
      "\u001b[32m2024-08-27 21:42:03.035\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mspeculative_rag\u001b[0m:\u001b[36m62\u001b[0m - \u001b[1mRAG Drafting done in 3.2212 seconds.\u001b[0m\n",
      "\u001b[32m2024-08-27 21:42:03.035\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mspeculative_rag\u001b[0m:\u001b[36m65\u001b[0m - \u001b[1mDoing RAG Verification...\u001b[0m\n",
      "\u001b[32m2024-08-27 21:42:03.846\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mspeculative_rag\u001b[0m:\u001b[36m82\u001b[0m - \u001b[1mRAG Verification done in 0.8099 seconds.\u001b[0m\n",
      "\u001b[32m2024-08-27 21:42:03.846\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mspeculative_rag\u001b[0m:\u001b[36m88\u001b[0m - \u001b[1mEntire process done in 4.5146 seconds.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Query2doc is a method designed to enhance information retrieval by leveraging large language models (LLMs) for query expansion. It operates by prompting LLMs with few-shot examples to generate pseudo-documents that are then integrated with existing sparse or dense retrieval systems. The primary goal of Query2doc is to distill the knowledge encoded in LLMs through this prompting process, thereby improving the retrieval performance across various models and datasets. Empirical evaluations have shown that this approach consistently leads to better retrieval results, although it does come with some limitations regarding efficiency and latency due to the nature of LLM inference.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing\n",
    "await speculative_rag.ainvoke({\"query\": \"What is Query2doc?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent (Graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[BaseMessage], add]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def call_model(state: AgentState) -> dict[str, list[BaseMessage]]:\n",
    "    \"\"\"Main function to invoke LLM\"\"\"\n",
    "    llm: ChatOpenAI = ChatOpenAI(\n",
    "        model=\"gpt-4o-2024-08-06\", temperature=0.0, streaming=True\n",
    "    ).bind_tools(tools=[speculative_rag])\n",
    "    prompt: ChatPromptTemplate = ChatPromptTemplate.from_messages(\n",
    "        messages=[\n",
    "            (\n",
    "                \"system\",\n",
    "                \"You are a helpful assistant. For any questions about Large Language Models (LLMs) and related use `get_relevant_information` tool.\",\n",
    "            ),\n",
    "            (\"placeholder\", \"{messages}\"),\n",
    "        ]\n",
    "    )\n",
    "    runnable: RunnableSequence = prompt | llm\n",
    "    response: BaseMessage = await runnable.ainvoke(\n",
    "        input={\"messages\": state.get(\"messages\", [])}\n",
    "    )\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "def should_continue(state: AgentState) -> Literal[\"speculative_rag\", END]:\n",
    "    last_message: BaseMessage = state.get(\"messages\", [])[-1]\n",
    "    if last_message.tool_calls:\n",
    "        logger.info(last_message.tool_calls)\n",
    "        return \"speculative_rag\"\n",
    "    return END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncompiled_graph: StateGraph = StateGraph(state_schema=AgentState)\n",
    "uncompiled_graph.add_node(node=\"agent\", action=call_model)\n",
    "uncompiled_graph.add_node(\n",
    "    node=\"speculative_rag\",\n",
    "    action=ToolNode(tools=[speculative_rag]),\n",
    ")\n",
    "uncompiled_graph.set_entry_point(key=\"agent\")\n",
    "uncompiled_graph.add_conditional_edges(source=\"agent\", path=should_continue)\n",
    "uncompiled_graph.add_edge(start_key=\"speculative_rag\", end_key=\"agent\")\n",
    "compiled_graph: CompiledStateGraph = uncompiled_graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(compiled_graph.get_graph().draw_mermaid_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-27 21:45:37.006\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mshould_continue\u001b[0m:\u001b[36m25\u001b[0m - \u001b[1m[{'name': 'get_relevant_information', 'args': {'query': 'Query2doc'}, 'id': 'call_cFyZUW75f7Nq2B7zlptXBBtB', 'type': 'tool_call'}]\u001b[0m\n",
      "\u001b[32m2024-08-27 21:45:37.052\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mspeculative_rag\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mGenerating query vector...\u001b[0m\n",
      "\u001b[32m2024-08-27 21:45:37.511\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mspeculative_rag\u001b[0m:\u001b[36m26\u001b[0m - \u001b[1mQuery vector generated in 0.4582 seconds.\u001b[0m\n",
      "\u001b[32m2024-08-27 21:45:37.511\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mspeculative_rag\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mFetching relevant documents...\u001b[0m\n",
      "\u001b[32m2024-08-27 21:45:37.513\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mspeculative_rag\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1mDocuments retrieved in 0.0013 seconds.\u001b[0m\n",
      "\u001b[32m2024-08-27 21:45:37.513\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mspeculative_rag\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mDoing Multi Perspective Sampling...\u001b[0m\n",
      "\u001b[32m2024-08-27 21:45:37.514\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmulti_perspective_sampling\u001b[0m:\u001b[36m5\u001b[0m - \u001b[1mFinding 2 clusters.\u001b[0m\n",
      "\u001b[32m2024-08-27 21:45:37.517\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmulti_perspective_sampling\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mClusters distribution: {1: [0, 2, 3, 4, 7, 9], 0: [1, 5, 6, 8]}\u001b[0m\n",
      "\u001b[32m2024-08-27 21:45:37.521\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmulti_perspective_sampling\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1m4 document subsets will be created.\u001b[0m\n",
      "\u001b[32m2024-08-27 21:45:37.521\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mspeculative_rag\u001b[0m:\u001b[36m42\u001b[0m - \u001b[1mMulti Perspective Sampling done in 0.0074 seconds.\u001b[0m\n",
      "\u001b[32m2024-08-27 21:45:37.522\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mspeculative_rag\u001b[0m:\u001b[36m47\u001b[0m - \u001b[1mDoing RAG Drafting...\u001b[0m\n",
      "\u001b[32m2024-08-27 21:45:44.001\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mspeculative_rag\u001b[0m:\u001b[36m62\u001b[0m - \u001b[1mRAG Drafting done in 6.4785 seconds.\u001b[0m\n",
      "\u001b[32m2024-08-27 21:45:44.002\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mspeculative_rag\u001b[0m:\u001b[36m65\u001b[0m - \u001b[1mDoing RAG Verification...\u001b[0m\n",
      "\u001b[32m2024-08-27 21:45:44.357\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mspeculative_rag\u001b[0m:\u001b[36m82\u001b[0m - \u001b[1mRAG Verification done in 0.3548 seconds.\u001b[0m\n",
      "\u001b[32m2024-08-27 21:45:44.357\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mspeculative_rag\u001b[0m:\u001b[36m88\u001b[0m - \u001b[1mEntire process done in 7.3480 seconds.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query2doc is a method that uses large language models (LLMs) for query expansion by generating pseudo-documents from the original query. This approach is designed to enhance the performance of existing retrieval systems by adding generated content. Despite its simplicity, query2doc has been shown to consistently improve retrieval outcomes across different models and datasets. However, it does introduce some challenges, such as increased latency due to the slow inference of LLMs and potentially longer search times in inverted indexes. Therefore, while it offers significant benefits, practical deployment must consider these efficiency trade-offs."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-27 21:45:45.650\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m10\u001b[0m - \u001b[1mTime to generate response:  9.3337 seconds.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "_now = perf_counter()\n",
    "async for event in  compiled_graph.astream_events(\n",
    "    input={\"messages\": [(\"user\", \"What is Query2doc?\")]},\n",
    "    version=\"v2\"\n",
    "):\n",
    "    kind: str = event.get(\"event\")\n",
    "    if kind == \"on_chat_model_stream\":\n",
    "        if token := event.get(\"data\").get(\"chunk\").content:\n",
    "            print(token, end=\"\", flush=True) \n",
    "logger.info(\"Time to generate response:  {s:.4f} seconds.\", s=perf_counter() - _now)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
