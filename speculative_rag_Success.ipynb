{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3Y93LpbRipI"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lO88VPBcRipJ"
      },
      "source": [
        "#### Libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    # from google.colab import userdata\n",
        "    # import os\n",
        "\n",
        "    # os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "Vs3uLslWSCbx"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3f1fea5",
        "outputId": "6cc3cd00-13d1-4cda-ec35-c108a05f4fd8"
      },
      "source": [
        "%pip install loguru"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting loguru\n",
            "  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
            "Downloading loguru-0.7.3-py3-none-any.whl (61 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/61.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: loguru\n",
            "Successfully installed loguru-0.7.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6853934a",
        "outputId": "e4a4c7d5-fab4-4ae8-dc42-52ae93b470a1"
      },
      "source": [
        "%pip install qdrant-client"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting qdrant-client\n",
            "  Downloading qdrant_client-1.15.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: grpcio>=1.41.0 in /usr/local/lib/python3.12/dist-packages (from qdrant-client) (1.75.0)\n",
            "Requirement already satisfied: httpx>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from httpx[http2]>=0.20.0->qdrant-client) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.26 in /usr/local/lib/python3.12/dist-packages (from qdrant-client) (2.0.2)\n",
            "Collecting portalocker<4.0,>=2.7.0 (from qdrant-client)\n",
            "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: protobuf>=3.20.0 in /usr/local/lib/python3.12/dist-packages (from qdrant-client) (5.29.5)\n",
            "Requirement already satisfied: pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8 in /usr/local/lib/python3.12/dist-packages (from qdrant-client) (2.11.9)\n",
            "Requirement already satisfied: urllib3<3,>=1.26.14 in /usr/local/lib/python3.12/dist-packages (from qdrant-client) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions~=4.12 in /usr/local/lib/python3.12/dist-packages (from grpcio>=1.41.0->qdrant-client) (4.15.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (0.16.0)\n",
            "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.12/dist-packages (from httpx[http2]>=0.20.0->qdrant-client) (4.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client) (0.4.1)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (4.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (1.3.1)\n",
            "Downloading qdrant_client-1.15.1-py3-none-any.whl (337 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m337.3/337.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: portalocker, qdrant-client\n",
            "Successfully installed portalocker-3.2.0 qdrant-client-1.15.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "9Z2GqVlERipJ"
      },
      "outputs": [],
      "source": [
        "import asyncio\n",
        "import json\n",
        "from collections import defaultdict\n",
        "from pathlib import Path\n",
        "from statistics import mean\n",
        "from time import perf_counter\n",
        "from typing import Any\n",
        "from uuid import uuid4\n",
        "\n",
        "import numpy as np\n",
        "from datasets import Dataset, load_dataset\n",
        "from loguru import logger\n",
        "from openai import AsyncOpenAI\n",
        "from pydantic import BaseModel, Field\n",
        "from sklearn.cluster import KMeans\n",
        "from tiktoken import Encoding, encoding_for_model, get_encoding\n",
        "\n",
        "from qdrant_client import AsyncQdrantClient, models\n",
        "\n",
        "#later\n",
        "import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpduOeonRipK"
      },
      "source": [
        "## 1. Create Qdrant collection and retriever"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3HPQcNDRipK"
      },
      "source": [
        "#### Initialize Clients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "p8LJab00RipK"
      },
      "outputs": [],
      "source": [
        "# Qdrant Client\n",
        "path: Path = Path(\"qdrant_client\")\n",
        "qdrant_client: AsyncQdrantClient = AsyncQdrantClient(path=path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "92JGx6HORipK"
      },
      "outputs": [],
      "source": [
        "# OpenAI Client\n",
        "openai_client: AsyncOpenAI = AsyncOpenAI()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Q9I9vH5RipK"
      },
      "source": [
        "#### Create collection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ChVtkw0XRipL"
      },
      "outputs": [],
      "source": [
        "# Embeddings specs\n",
        "embedding_model: str = \"text-embedding-3-small\"\n",
        "dimension: int = 1536\n",
        "collection_name: str = \"speculative_rag\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydo7fMigRipL",
        "outputId": "f6c02922-39ed-4e16-a5d8-434188b47caf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m2025-09-24 15:45:26.394\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 1>\u001b[0m:\u001b[36m6\u001b[0m - \u001b[1mCollection speculative_rag doesn't exist. Creating...\u001b[0m\n",
            "\u001b[32m2025-09-24 15:45:26.412\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 1>\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mCollection speculative_rag created!\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Get existing collections\n",
        "current_collections: models.CollectionsResponse = await qdrant_client.get_collections()\n",
        "\n",
        "# Create collection\n",
        "if collection_name not in [col.name for col in current_collections.collections]:\n",
        "    logger.info(\"Collection {col} doesn't exist. Creating...\", col=collection_name)\n",
        "    await qdrant_client.create_collection(\n",
        "        collection_name=collection_name,\n",
        "        vectors_config=models.VectorParams(\n",
        "            size=dimension, distance=models.Distance.DOT\n",
        "        ),\n",
        "    )\n",
        "    logger.info(\"Collection {col} created!\", col=collection_name)\n",
        "else:\n",
        "    logger.info(\n",
        "        \"Collection {col} already exists, skipping creation.\", col=collection_name\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLgoDrfDRipL"
      },
      "source": [
        "#### Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415,
          "referenced_widgets": [
            "28679893917548189fd79018ff7d1cf4",
            "85275696b30247928a2209f2837ba926",
            "7b667505084c4274ab80385aaa4e8c1d",
            "e8db2ebf1708464b891e4665b219adbf",
            "436726c28c884e68a660255160db7198",
            "783b9e824b2e4e7282db68d3a49c6d27",
            "06abf32f8e1d4c22a5ccd39e684838cc",
            "a665cf280c404342b5cf8ddaa3601f72",
            "8aecdd00aa34456ca4ea540fca48a2ce",
            "865c8fbde599442eb2a6aa7ee10f21fc",
            "88c553ff3c924d2b92b43af3c1cc0b0c",
            "5477310c81344d4a87318d8a3ad8f29d",
            "b04d25cb4c1d4e688c190a98355952ac",
            "e0d62bbe98e74c9c8f86df05effe177d",
            "5faf179ef8274fb9a9082d349f603d51",
            "680c541b023546258798d94985a9f80f",
            "cfa4a48078ec46ff84f20eea0ece07d7",
            "a44fa85d031a4e7fb34abf11aca5fdc0",
            "581ad864944248a0a116130df974d3b4",
            "f71affce762c4ee98ca000920875f511",
            "39a5a6f716f84c8fa35cda546383b596",
            "ee42ab433eaf48cfb479578a77369df6"
          ]
        },
        "id": "H4AlkwO6RipL",
        "outputId": "da74936c-695e-48f2-a1c0-5aa5d7be0c5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train.jsonl:   0%|          | 0.00/253M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "28679893917548189fd79018ff7d1cf4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/209760 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5477310c81344d4a87318d8a3ad8f29d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"id\": \"2401.04088#0\",\n",
            "    \"title\": \"Mixtral of Experts\",\n",
            "    \"content\": \"4 2 0 2 n a J 8 ] G L . s c [ 1 v 8 8 0 4 0 . 1 0 4 2 : v i X r a # Mixtral of Experts Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, L\\u00c3\\u00a9lio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Th\\u00c3\\u00a9ophile Gervet, Thibaut Lavril, Thomas Wang, Timoth\\u00c3\\u00a9e Lacroix, William El Sayed Abstract We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine- tuned to follow instructions, Mixtral 8x7B \\u00e2 Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B \\u00e2 chat model on human bench- marks. Both the base and instruct models are released under the Apache 2.0 license.\",\n",
            "    \"prechunk_id\": \"\",\n",
            "    \"postchunk_id\": \"2401.04088#1\",\n",
            "    \"arxiv_id\": \"2401.04088\",\n",
            "    \"references\": [\n",
            "        \"1905.07830\"\n",
            "    ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# Load dataset\n",
        "dataset: Dataset = load_dataset(\n",
        "    path=\"jamescalam/ai-arxiv2-semantic-chunks\", split=\"train\"\n",
        ")\n",
        "print(json.dumps(dataset[0], indent=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "cVFs1PrIRipL"
      },
      "outputs": [],
      "source": [
        "# Using only 50k rows\n",
        "rows_to_keep: int = 2\n",
        "\n",
        "# Easier to handle as pandas df\n",
        "records: list[dict[str, Any]] = (\n",
        "    dataset.to_pandas().iloc[:rows_to_keep].to_dict(orient=\"records\")\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCH0DI7FRipL",
        "outputId": "7f4a8807-6658-498a-e8ed-0a518068ba62"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': '2401.04088#0',\n",
              " 'title': 'Mixtral of Experts',\n",
              " 'content': '4 2 0 2 n a J 8 ] G L . s c [ 1 v 8 8 0 4 0 . 1 0 4 2 : v i X r a # Mixtral of Experts Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, LÃ©lio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, ThÃ©ophile Gervet, Thibaut Lavril, Thomas Wang, TimothÃ©e Lacroix, William El Sayed Abstract We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine- tuned to follow instructions, Mixtral 8x7B â Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â chat model on human bench- marks. Both the base and instruct models are released under the Apache 2.0 license.',\n",
              " 'prechunk_id': '',\n",
              " 'postchunk_id': '2401.04088#1',\n",
              " 'arxiv_id': '2401.04088',\n",
              " 'references': array(['1905.07830'], dtype=object)}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "records[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4rEdUKuRipL"
      },
      "source": [
        "#### Upload information to Qdrant (run only once!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "9jg4XriKRipL"
      },
      "outputs": [],
      "source": [
        "# Auxiliar functions to prepare the Points\n",
        "async def create_point(\n",
        "    client: AsyncOpenAI,\n",
        "    example: dict[str, Any],\n",
        "    model: str,\n",
        "    encoding_name: str,\n",
        "    max_context_len: int,\n",
        ") -> models.PointStruct:\n",
        "    \"\"\"Creates a Point that contains the payload and the vector.\"\"\"\n",
        "\n",
        "    encoding: Encoding = get_encoding(encoding_name=encoding_name)\n",
        "\n",
        "    embedding_result: Any = await client.embeddings.create(\n",
        "        input=encoding.encode(text=example.get(\"content\"), disallowed_special=())[\n",
        "            :max_context_len\n",
        "        ],\n",
        "        model=model,\n",
        "    )\n",
        "    vector: list[float] = embedding_result.data[0].embedding\n",
        "\n",
        "    return models.PointStruct(\n",
        "        id=str(uuid4()),\n",
        "        vector=vector,\n",
        "        payload=dict(\n",
        "            chunk_id=example.get(\"id\"),\n",
        "            arxiv_id=example.get(\"arxiv_id\"),\n",
        "            title=example.get(\"title\"),\n",
        "            content=example.get(\"content\"),\n",
        "            prechunk_id=example.get(\"prechunk_id\"),\n",
        "            postchunk_id=example.get(\"postchunk_id\"),\n",
        "            references=example.get(\"references\").tolist(),\n",
        "        ),\n",
        "    )\n",
        "\n",
        "\n",
        "async def process_batch(\n",
        "    client: AsyncOpenAI,\n",
        "    batch: list[dict[str, Any]],\n",
        "    model: str,\n",
        "    encoding_name: str,\n",
        "    max_context_len: int,\n",
        ") -> list[models.PointStruct]:\n",
        "    \"\"\"Processes a batch of examples to create PointStructs.\"\"\"\n",
        "    return await asyncio.gather(\n",
        "        *[\n",
        "            create_point(\n",
        "                client=client,\n",
        "                example=example,\n",
        "                model=model,\n",
        "                encoding_name=encoding_name,\n",
        "                max_context_len=max_context_len,\n",
        "            )\n",
        "            for example in batch\n",
        "        ]\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbrrqUbGRipL",
        "outputId": "d7ce977c-f190-4114-a1df-c685511e8acb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Points: 1it [00:04,  4.14s/it]\n",
            "\u001b[32m2025-09-24 15:46:03.374\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 0>\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1mGenerated all Points in 4.1455 seconds.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "batch_size: int = 512\n",
        "max_context_len: int = 8192\n",
        "encoding_name: str = \"cl100k_base\"\n",
        "total_batches: int = len(records) // batch_size\n",
        "all_points: list[models.PointStruct | None] = []\n",
        "\n",
        "_now: float = perf_counter()\n",
        "for i in tqdm.tqdm(range(0, len(records), batch_size), total=total_batches, desc=\"Points\"):\n",
        "    batch: list[dict[str, Any]] = records[i : i + batch_size]\n",
        "    points: list[models.PointStruct] = await process_batch(\n",
        "        client=openai_client,\n",
        "        batch=batch,\n",
        "        model=embedding_model,\n",
        "        encoding_name=encoding_name,\n",
        "        max_context_len=max_context_len,\n",
        "    )\n",
        "    all_points.extend(points)\n",
        "logger.info(\"Generated all Points in {secs:.4f} seconds.\", secs=perf_counter() - _now)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHh7xIvhRipL",
        "outputId": "2d90f117-c17b-48d4-9738-a26cb8d7f300"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "UpdateResult(operation_id=0, status=<UpdateStatus.COMPLETED: 'completed'>)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "# Upsert Points\n",
        "await qdrant_client.upsert(collection_name=collection_name, points=all_points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lj6lolrKRipM"
      },
      "source": [
        "#### testing vector search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njEgw-wjRipM",
        "outputId": "8167b938-b7bc-4701-e7c1-e8bee2802864"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4214118903.py:6: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
            "  out: list[models.ScoredPoint] = await qdrant_client.search(\n"
          ]
        }
      ],
      "source": [
        "query: str = \"Mixture of Experts\"\n",
        "query_vector: Any = await openai_client.embeddings.create(\n",
        "    input=query, model=embedding_model\n",
        ")\n",
        "query_vector: list[float] = query_vector.data[0].embedding\n",
        "out: list[models.ScoredPoint] = await qdrant_client.search(\n",
        "    collection_name=collection_name, query_vector=query_vector, with_vectors=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMMsuELgRipM",
        "outputId": "e710adb6-103d-4d02-cc7b-38f8ae225cdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Id: 8c63a784-8e85-4d87-9f4e-8ed93eb2a0e4\n",
            "Score: 0.556\n",
            "Title: Mixtral of Experts [2401.04088]\n",
            "Chunk: Code: https://github.com/mistralai/mistral-src Webpage: https://mistral.ai/news/mixtral-of-experts/ # Introduction In this paper, we present Mixtral 8x7B, a sparse mixture of experts model (SMoE) with open weights, licensed under Apache 2.0. Mixtral outperforms Llama 2 70B and GPT-3.5 on most benchmarks. As it only uses a subset of its parameters for every token, Mixtral allows faster inference speed at low batch-sizes, and higher throughput at large batch-sizes. Mixtral is a sparse mixture-of-experts network. It is a decoder-only model where the feedforward block picks from a set of 8 distinct groups of parameters. At every layer, for every token, a router network chooses two of these groups (the â ...\n",
            "Vector: [-0.0048737297765910625, 0.006692185997962952, 0.017368929460644722, -0.03131488338112831, -0.007293880917131901] ... \n"
          ]
        }
      ],
      "source": [
        "print(f\"Id: {out[0].id}\")\n",
        "print(f\"Score: {out[0].score:.3}\")\n",
        "print(f\"Title: {out[0].payload.get('title')} [{out[0].payload.get('arxiv_id')}]\")\n",
        "print(f\"Chunk: {out[0].payload.get('content')[:1000]} ...\")\n",
        "print(f\"Vector: {out[0].vector[:5]} ... \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INcYiKqoRipM"
      },
      "source": [
        "## 2. Speculative RAG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ms54ppCMRipM"
      },
      "source": [
        "#### Multi-Perspective Sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "99KRanxKRipM"
      },
      "outputs": [],
      "source": [
        "def multi_perspective_sampling(\n",
        "    k: int, retrieved_points: list[models.ScoredPoint], seed: int = 1399\n",
        ") -> list[list[str]]:\n",
        "    # Generate clusters\n",
        "    logger.info(\"Finding {k} clusters.\", k=k)\n",
        "    algo: Any = KMeans(n_clusters=k, random_state=seed)\n",
        "    _vectors = [point.vector for point in retrieved_points]\n",
        "    clusters: list[int] = algo.fit_predict(X=_vectors)\n",
        "\n",
        "    # Unique clusters\n",
        "    unique_clusters: set[int] = set(clusters)\n",
        "\n",
        "    # Create a dictionary with the members of each cluster\n",
        "    cluster_dict: defaultdict[int, list[int | None]] = defaultdict(list)\n",
        "    for index, cluster in enumerate(clusters):\n",
        "        cluster_dict[cluster].append(index)\n",
        "    logger.info(\"Clusters distribution: {dist}\", dist=dict(cluster_dict))\n",
        "\n",
        "    # M subsets\n",
        "    m: int = min(len(indices) for indices in cluster_dict.values())\n",
        "    logger.info(\"{m} document subsets will be created.\", m=m)\n",
        "\n",
        "    # Generate m unique subsets without replacement\n",
        "    np.random.seed(seed=seed)\n",
        "    subsets: list[list[str]] = []\n",
        "\n",
        "    for _ in range(m):\n",
        "        subset: list[int] = []\n",
        "        for cluster in unique_clusters:\n",
        "            chosen_element: int = np.random.choice(cluster_dict[cluster])\n",
        "            subset.append(chosen_element)\n",
        "            cluster_dict[cluster].remove(chosen_element)\n",
        "        subset_documents = [\n",
        "            retrieved_points[idx].payload.get(\"content\") for idx in subset\n",
        "        ]\n",
        "        subsets.append(subset_documents)\n",
        "\n",
        "    return subsets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1FdoP0qARipM",
        "outputId": "19245808-5fea-4270-824d-8048e3353719"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m2025-09-24 15:49:23.572\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmulti_perspective_sampling\u001b[0m:\u001b[36m5\u001b[0m - \u001b[1mFinding 2 clusters.\u001b[0m\n",
            "\u001b[32m2025-09-24 15:49:23.650\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmulti_perspective_sampling\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mClusters distribution: {np.int32(0): [0], np.int32(1): [1]}\u001b[0m\n",
            "\u001b[32m2025-09-24 15:49:23.651\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmulti_perspective_sampling\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1m1 document subsets will be created.\u001b[0m\n",
            "\u001b[32m2025-09-24 15:49:23.653\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 0>\u001b[0m:\u001b[36m8\u001b[0m - \u001b[1mMulti perspective sampling done in 0.0807 seconds.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Testing\n",
        "k: int = 2\n",
        "seed: int = 1399\n",
        "now: float = perf_counter()\n",
        "sampled_docs: list[list[str]] = multi_perspective_sampling(\n",
        "    k=k, retrieved_points=out, seed=seed\n",
        ")\n",
        "logger.info(\n",
        "    \"Multi perspective sampling done in {s:.4f} seconds.\", s=perf_counter() - now\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVfLiDYPRipM",
        "outputId": "e8f46386-ec01-4cb2-ff04-0eaf72e07cf2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Code: https://github.com/mistralai/mistral-src Webpage: https://mistral.ai/news/mixtral-of-experts/ # Introduction In this paper, we present Mixtral 8x7B, a sparse mixture of experts model (SMoE) with open weights, licensed under Apache 2.0. Mixtral outperforms Llama 2 70B and GPT-3.5 on most benchmarks. As it only uses a subset of its parameters for every token, Mixtral allows faster inference speed at low batch-sizes, and higher throughput at large batch-sizes. Mixtral is a sparse mixture-of-experts network. It is a decoder-only model where the feedforward block picks from a set of 8 distinct groups of parameters. At every layer, for every token, a router network chooses two of these groups (the â',\n",
              "  '4 2 0 2 n a J 8 ] G L . s c [ 1 v 8 8 0 4 0 . 1 0 4 2 : v i X r a # Mixtral of Experts Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, LÃ©lio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, ThÃ©ophile Gervet, Thibaut Lavril, Thomas Wang, TimothÃ©e Lacroix, William El Sayed Abstract We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine- tuned to follow instructions, Mixtral 8x7B â Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â chat model on human bench- marks. Both the base and instruct models are released under the Apache 2.0 license.']]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "sampled_docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQgutKPLRipM"
      },
      "source": [
        "#### Rag Drafting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "BCYpna7RRipM"
      },
      "outputs": [],
      "source": [
        "rag_drafting_prompt: str = \"\"\"Response to the instruction. Also provide rationale for your response.\n",
        "## Instruction: {instruction}\n",
        "\n",
        "## Evidence: {evidence}\"\"\"\n",
        "\n",
        "\n",
        "class RagDraftingResponse(BaseModel):\n",
        "    rationale: str = Field(description=\"Response rationale.\")\n",
        "    response: str = Field(description=\"Response to the instruction.\")\n",
        "\n",
        "\n",
        "async def rag_drafting_generator(\n",
        "    client: AsyncOpenAI,\n",
        "    model_name: str,\n",
        "    instruction: str,\n",
        "    evidence: str,\n",
        "    **kwargs,\n",
        ") -> tuple[RagDraftingResponse, float]:\n",
        "    completion: Any = await client.beta.chat.completions.parse(\n",
        "        model=model_name,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": rag_drafting_prompt.format(\n",
        "                    instruction=instruction, evidence=evidence\n",
        "                ),\n",
        "            }\n",
        "        ],\n",
        "        response_format=RagDraftingResponse,\n",
        "        temperature=0.0,\n",
        "        logprobs=True,\n",
        "        max_tokens=512,\n",
        "        **kwargs,\n",
        "    )\n",
        "    return (\n",
        "        completion.choices[0].message.parsed,\n",
        "        np.exp(mean(token.logprob for token in completion.choices[0].logprobs.content)),\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rmushCmRipM",
        "outputId": "9c081c77-eeaf-45ce-fa99-00e71c26dc03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m2025-09-24 15:51:17.591\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 0>\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mRAG Drafting done in 6.5073 seconds.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(RagDraftingResponse(rationale='The response provides a clear definition of MoE (Mixture of Experts) by explaining its structure and functionality, particularly in the context of the Mixtral model. It highlights how MoE operates by using a subset of parameters for each token, which enhances efficiency and performance. This aligns with the instruction to explain what MoE is, while also incorporating relevant details from the provided evidence.', response='MoE, or Mixture of Experts, is a machine learning architecture that utilizes a subset of its parameters for processing each input token. In the context of the Mixtral model, which is a Sparse Mixture of Experts (SMoE), the architecture consists of multiple feedforward blocks (or experts) at each layer. For every token, a router network selects two of these experts to process the input, allowing the model to leverage a larger number of parameters (47 billion) while only activating a smaller subset (13 billion) during inference. This design enables faster processing speeds and improved performance on various benchmarks.'),\n",
              "  np.float64(0.7266243146935542))]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "# Testing\n",
        "m_drafter: str = \"gpt-4o-mini-2024-07-18\"\n",
        "instruction: str = \"What is MoE?\"\n",
        "\n",
        "now: float = perf_counter()\n",
        "rag_drafts: list[tuple[RagDraftingResponse, float]] = await asyncio.gather(\n",
        "    *[\n",
        "        rag_drafting_generator(\n",
        "            client=openai_client,\n",
        "            model_name=m_drafter,\n",
        "            instruction=instruction,\n",
        "            evidence=\"\\n\".join(\n",
        "                [f\"[{idx}] {doc}\" for idx, doc in enumerate(subset, start=1)]\n",
        "            ),\n",
        "        )\n",
        "        for subset in sampled_docs\n",
        "    ]\n",
        ")\n",
        "logger.info(\"RAG Drafting done in {s:.4f} seconds.\", s=perf_counter() - now)\n",
        "rag_drafts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfGNenbcRipM"
      },
      "source": [
        "#### Generalist RAG Verifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "O8pwKBU-RipM"
      },
      "outputs": [],
      "source": [
        "rag_verifier_prompt: str = \"\"\"## Instruction: {instruction}\n",
        "\n",
        "## Response: {response}\n",
        "\n",
        "## Rationale: {rationale}\n",
        "\n",
        "Is the rationale good enough to support the answer? (Yes or No)\"\"\"\n",
        "\n",
        "\n",
        "async def rag_verifier_generator(\n",
        "    client: AsyncOpenAI,\n",
        "    model_name: str,\n",
        "    instruction: str,\n",
        "    evidence: str,\n",
        "    response: str,\n",
        "    rationale: str,\n",
        "    **kwargs,\n",
        ") -> tuple[Any, float]:\n",
        "    encoder: Encoding = encoding_for_model(model_name=model_name)\n",
        "    completion: Any = await client.chat.completions.create(\n",
        "        model=model_name,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": rag_verifier_prompt.format(\n",
        "                    instruction=instruction,\n",
        "                    evidence=evidence,\n",
        "                    response=response,\n",
        "                    rationale=rationale,\n",
        "                ),\n",
        "            }\n",
        "        ],\n",
        "        temperature=0.0,\n",
        "        logprobs=True,\n",
        "        max_tokens=2,\n",
        "        **kwargs,\n",
        "    )\n",
        "    response: str = completion.choices[0].message.content\n",
        "    cond: bool = encoder.encode(text=response.lower()) == encoder.encode(text=\"yes\")\n",
        "    p_yes: float = (\n",
        "        np.exp(mean(token.logprob for token in completion.choices[0].logprobs.content))\n",
        "        if cond\n",
        "        else 0.0\n",
        "    )  # Naive\n",
        "\n",
        "    return (response, p_yes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMHzmQCdRipM",
        "outputId": "4514cb8d-37a1-4a0b-9ba3-5907e12e702c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m2025-09-24 15:52:15.689\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 0>\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mRAG Drafting done in 2.7521 seconds.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Yes', np.float64(0.9999797803764172))]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "# Testing\n",
        "m_verifier: str = \"gpt-4o-2024-08-06\"\n",
        "instruction: str = \"What is MoE?\"\n",
        "\n",
        "now: float = perf_counter()\n",
        "rag_verifications: list[tuple[str, float]] = await asyncio.gather(\n",
        "    *[\n",
        "        rag_verifier_generator(\n",
        "            client=openai_client,\n",
        "            model_name=m_verifier,\n",
        "            instruction=instruction,\n",
        "            evidence=\"\\n\".join(\n",
        "                [f\"[{idx}] {doc}\" for idx, doc in enumerate(subset, start=1)]\n",
        "            ),\n",
        "            response=rag_drafting_response.response,\n",
        "            rationale=rag_drafting_response.rationale,\n",
        "        )\n",
        "        for subset, (rag_drafting_response, _) in zip(sampled_docs, rag_drafts)\n",
        "    ]\n",
        ")\n",
        "logger.info(\"RAG Drafting done in {s:.4f} seconds.\", s=perf_counter() - now)\n",
        "rag_verifications"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYTruNZTRipN"
      },
      "source": [
        "#### Final Response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XNAAWRsRipN",
        "outputId": "f928fd7f-a16b-47a9-b369-9fac915b2632"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            " ------ \n",
            "MoE, or Mixture of Experts, is a machine learning architecture that utilizes a subset of its parameters for processing each input token. In the context of the Mixtral model, which is a Sparse Mixture of Experts (SMoE), the architecture consists of multiple feedforward blocks (or experts) at each layer. For every token, a router network selects two of these experts to process the input, allowing the model to leverage a larger number of parameters (47 billion) while only activating a smaller subset (13 billion) during inference. This design enables faster processing speeds and improved performance on various benchmarks.\n"
          ]
        }
      ],
      "source": [
        "best_answer: int = np.argmax(\n",
        "    p_draft * p_self for (_, p_draft), (_, p_self) in zip(rag_drafts, rag_verifications)\n",
        ")\n",
        "print(f\"Response:\\n ------ \\n{rag_drafts[best_answer][0].response}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Za6kCjfcRipN"
      },
      "source": [
        "## 3. \"end-to-end\" Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfPzpXtMRipN"
      },
      "source": [
        "#### Speculative Rag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "A0dE5UcLRipN"
      },
      "outputs": [],
      "source": [
        "async def speculative_rag(\n",
        "    query: str,\n",
        "    embedding_model: str,\n",
        "    collection_name: str,\n",
        "    k: int,\n",
        "    seed: int,\n",
        "    client: AsyncOpenAI,\n",
        "    qdrant_client: AsyncQdrantClient,\n",
        "    m_drafter: str,\n",
        "    m_verifier: str,\n",
        ") -> str:\n",
        "    _start = perf_counter()\n",
        "\n",
        "    # Generate query vector embedding\n",
        "    logger.info(\"Generating query vector...\")\n",
        "    _now: float = perf_counter()\n",
        "    query_vector: Any = await client.embeddings.create(\n",
        "        input=query, model=embedding_model\n",
        "    )\n",
        "    query_vector: list[float] = query_vector.data[0].embedding\n",
        "    logger.info(\"Query vector generated in {s:.4f} seconds.\", s=perf_counter() - _now)\n",
        "\n",
        "    # Fetching relevant documents\n",
        "    logger.info(\"Fetching relevant documents...\")\n",
        "    _now: float = perf_counter()\n",
        "    out: list[models.ScoredPoint] = await qdrant_client.search(\n",
        "        collection_name=collection_name, query_vector=query_vector, with_vectors=True\n",
        "    )\n",
        "    logger.info(\"Documents retrieved in {s:.4f} seconds.\", s=perf_counter() - _now)\n",
        "\n",
        "    # Multi Perspective Sampling\n",
        "    logger.info(\"Doing Multi Perspective Sampling...\")\n",
        "    _now: float = perf_counter()\n",
        "    sampled_docs: list[list[str]] = multi_perspective_sampling(\n",
        "        k=k, retrieved_points=out, seed=seed\n",
        "    )\n",
        "    logger.info(\n",
        "        \"Multi Perspective Sampling done in {s:.4f} seconds.\", s=perf_counter() - _now\n",
        "    )\n",
        "\n",
        "    # RAG Drafting\n",
        "    logger.info(\"Doing RAG Drafting...\")\n",
        "    _now: float = perf_counter()\n",
        "    rag_drafts: list[tuple[RagDraftingResponse, float]] = await asyncio.gather(\n",
        "        *[\n",
        "            rag_drafting_generator(\n",
        "                client=client,\n",
        "                model_name=m_drafter,\n",
        "                instruction=query,\n",
        "                evidence=\"\\n\".join(\n",
        "                    [f\"[{idx}] {doc}\" for idx, doc in enumerate(subset, start=1)]\n",
        "                ),\n",
        "            )\n",
        "            for subset in sampled_docs\n",
        "        ]\n",
        "    )\n",
        "    logger.info(\"RAG Drafting done in {s:.4f} seconds.\", s=perf_counter() - _now)\n",
        "\n",
        "    # RAG Verifier\n",
        "    logger.info(\"Doing RAG Verification...\")\n",
        "    _now: float = perf_counter()\n",
        "    rag_verifications: list[tuple[str, float]] = await asyncio.gather(\n",
        "        *[\n",
        "            rag_verifier_generator(\n",
        "                client=client,\n",
        "                model_name=m_verifier,\n",
        "                instruction=query,\n",
        "                evidence=\"\\n\".join(\n",
        "                    [f\"[{idx}] {doc}\" for idx, doc in enumerate(subset, start=1)]\n",
        "                ),\n",
        "                response=rag_drafting_response.response,\n",
        "                rationale=rag_drafting_response.rationale,\n",
        "            )\n",
        "            for subset, (rag_drafting_response, _) in zip(sampled_docs, rag_drafts)\n",
        "        ]\n",
        "    )\n",
        "    logger.info(\"RAG Verification done in {s:.4f} seconds.\", s=perf_counter() - _now)\n",
        "\n",
        "    best_answer: int = np.argmax(\n",
        "        p_draft * p_self\n",
        "        for (_, p_draft), (_, p_self) in zip(rag_drafts, rag_verifications)\n",
        "    )\n",
        "    logger.info(\"Entire process done in {s:.4f} seconds.\", s=perf_counter() - _start)\n",
        "    print(f\"\\nQuestion:\\n ------ \\n{query}\\n\\n\")\n",
        "    print(f\"Response:\\n ------ \\n{rag_drafts[best_answer][0].response}\")\n",
        "    return rag_drafts[best_answer][0].response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SyflYZw1RipN",
        "outputId": "eabd6274-96ed-4e44-e4ea-ba64c7982f23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m2025-09-24 15:54:08.099\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mspeculative_rag\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1mGenerating query vector...\u001b[0m\n",
            "\u001b[32m2025-09-24 15:54:08.559\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mspeculative_rag\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mQuery vector generated in 0.4590 seconds.\u001b[0m\n",
            "\u001b[32m2025-09-24 15:54:08.560\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mspeculative_rag\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mFetching relevant documents...\u001b[0m\n",
            "/tmp/ipython-input-1154689518.py:26: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
            "  out: list[models.ScoredPoint] = await qdrant_client.search(\n",
            "\u001b[32m2025-09-24 15:54:08.564\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mspeculative_rag\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mDocuments retrieved in 0.0026 seconds.\u001b[0m\n",
            "\u001b[32m2025-09-24 15:54:08.565\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mspeculative_rag\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mDoing Multi Perspective Sampling...\u001b[0m\n",
            "\u001b[32m2025-09-24 15:54:08.565\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmulti_perspective_sampling\u001b[0m:\u001b[36m5\u001b[0m - \u001b[1mFinding 2 clusters.\u001b[0m\n",
            "\u001b[32m2025-09-24 15:54:08.574\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmulti_perspective_sampling\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mClusters distribution: {np.int32(0): [0], np.int32(1): [1]}\u001b[0m\n",
            "\u001b[32m2025-09-24 15:54:08.575\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmulti_perspective_sampling\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1m1 document subsets will be created.\u001b[0m\n",
            "\u001b[32m2025-09-24 15:54:08.577\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mspeculative_rag\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mMulti Perspective Sampling done in 0.0111 seconds.\u001b[0m\n",
            "\u001b[32m2025-09-24 15:54:08.577\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mspeculative_rag\u001b[0m:\u001b[36m42\u001b[0m - \u001b[1mDoing RAG Drafting...\u001b[0m\n",
            "\u001b[32m2025-09-24 15:54:12.045\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mspeculative_rag\u001b[0m:\u001b[36m57\u001b[0m - \u001b[1mRAG Drafting done in 3.4676 seconds.\u001b[0m\n",
            "\u001b[32m2025-09-24 15:54:12.046\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mspeculative_rag\u001b[0m:\u001b[36m60\u001b[0m - \u001b[1mDoing RAG Verification...\u001b[0m\n",
            "\u001b[32m2025-09-24 15:54:13.718\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mspeculative_rag\u001b[0m:\u001b[36m77\u001b[0m - \u001b[1mRAG Verification done in 1.6707 seconds.\u001b[0m\n",
            "\u001b[32m2025-09-24 15:54:13.719\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mspeculative_rag\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mEntire process done in 5.6203 seconds.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Question:\n",
            " ------ \n",
            "What is Query2doc?\n",
            "\n",
            "\n",
            "Response:\n",
            " ------ \n",
            "The provided evidence does not contain any information about Query2doc. It primarily discusses the Mixtral model, which is a Sparse Mixture of Experts language model. If you need information about Query2doc, please provide additional context or sources.\n"
          ]
        }
      ],
      "source": [
        "final_answer: str = await speculative_rag(\n",
        "    query=\"What is Query2doc?\",\n",
        "    embedding_model=embedding_model,\n",
        "    collection_name=collection_name,\n",
        "    k=k,\n",
        "    seed=seed,\n",
        "    client=openai_client,\n",
        "    qdrant_client=qdrant_client,\n",
        "    m_drafter=m_drafter,\n",
        "    m_verifier=m_verifier,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONHrXAJzRipN"
      },
      "source": [
        "#### Base RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "TKnxfPDhRipW"
      },
      "outputs": [],
      "source": [
        "async def base_rag(\n",
        "    query: str,\n",
        "    embedding_model: str,\n",
        "    collection_name: str,\n",
        "    client: AsyncOpenAI,\n",
        "    qdrant_client: AsyncQdrantClient,\n",
        "    generation_model: str,\n",
        ") -> str:\n",
        "    _start = perf_counter()\n",
        "\n",
        "    # Generate query vector embedding\n",
        "    logger.info(\"Generating query vector...\")\n",
        "    _now: float = perf_counter()\n",
        "    query_vector: Any = await client.embeddings.create(\n",
        "        input=query, model=embedding_model\n",
        "    )\n",
        "    query_vector: list[float] = query_vector.data[0].embedding\n",
        "    logger.info(\"Query vector generated in {s:.4f} seconds.\", s=perf_counter() - _now)\n",
        "\n",
        "    # Fetching relevant documents\n",
        "    logger.info(\"Fetching relevant documents...\")\n",
        "    _now: float = perf_counter()\n",
        "    out: list[models.ScoredPoint] = await qdrant_client.search(\n",
        "        collection_name=collection_name, query_vector=query_vector, with_vectors=True\n",
        "    )\n",
        "    logger.info(\"Documents retrieved in {s:.4f} seconds.\", s=perf_counter() - _now)\n",
        "\n",
        "    # Base RAG\n",
        "    logger.info(\"Generating response...\")\n",
        "    prompt: str = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "    ### Evidence: {evidence}\n",
        "\n",
        "    ### Instruction: {instruction}\n",
        "\n",
        "    ### Response:\"\"\"\n",
        "\n",
        "    completion: Any = await client.chat.completions.create(\n",
        "        model=generation_model,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": prompt.format(\n",
        "                    instruction=query,\n",
        "                    evidence=\"\\n\".join(\n",
        "                        [\n",
        "                            f\"[{idx}] {point.payload.get('content')}\"\n",
        "                            for idx, point in enumerate(out, start=1)\n",
        "                        ]\n",
        "                    ),\n",
        "                ),\n",
        "            }\n",
        "        ],\n",
        "        temperature=0.0,\n",
        "        logprobs=True,\n",
        "    )\n",
        "    response: str = completion.choices[0].message.content\n",
        "    logger.info(\"Response generated in {s:.4f} seconds.\", s=perf_counter() - _now)\n",
        "\n",
        "    logger.info(\"Entire process done in {s:.4f} seconds.\", s=perf_counter() - _start)\n",
        "    print(f\"\\nQuestion:\\n ------ \\n{query}\\n\\n\")\n",
        "    print(f\"Response:\\n ------ \\n{response}\")\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4JK6iHCRipW",
        "outputId": "bd69ccec-113f-417a-9918-9807a3dd7ed8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m2025-09-24 15:54:40.710\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mbase_rag\u001b[0m:\u001b[36m12\u001b[0m - \u001b[1mGenerating query vector...\u001b[0m\n",
            "\u001b[32m2025-09-24 15:54:41.151\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mbase_rag\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1mQuery vector generated in 0.4389 seconds.\u001b[0m\n",
            "\u001b[32m2025-09-24 15:54:41.152\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mbase_rag\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mFetching relevant documents...\u001b[0m\n",
            "/tmp/ipython-input-1577768163.py:23: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
            "  out: list[models.ScoredPoint] = await qdrant_client.search(\n",
            "\u001b[32m2025-09-24 15:54:41.155\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mbase_rag\u001b[0m:\u001b[36m26\u001b[0m - \u001b[1mDocuments retrieved in 0.0012 seconds.\u001b[0m\n",
            "\u001b[32m2025-09-24 15:54:41.156\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mbase_rag\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mGenerating response...\u001b[0m\n",
            "\u001b[32m2025-09-24 15:54:46.312\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mbase_rag\u001b[0m:\u001b[36m58\u001b[0m - \u001b[1mResponse generated in 5.1588 seconds.\u001b[0m\n",
            "\u001b[32m2025-09-24 15:54:46.313\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mbase_rag\u001b[0m:\u001b[36m60\u001b[0m - \u001b[1mEntire process done in 5.6032 seconds.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Question:\n",
            " ------ \n",
            "What is Query2doc?\n",
            "\n",
            "\n",
            "Response:\n",
            " ------ \n",
            "Query2doc is not explicitly mentioned in the provided evidence. However, based on the context of the evidence, Query2doc could potentially refer to a system or method related to processing or understanding queries in the context of language models or machine learning. It might involve converting queries into document-like representations or using a model to generate responses to queries. Without specific information from the evidence, this is a speculative interpretation.\n"
          ]
        }
      ],
      "source": [
        "final_answer: str = await base_rag(\n",
        "    query=\"What is Query2doc?\",\n",
        "    embedding_model=embedding_model,\n",
        "    collection_name=collection_name,\n",
        "    client=openai_client,\n",
        "    qdrant_client=qdrant_client,\n",
        "    generation_model=m_verifier,\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "colab": {
      "provenance": []
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "28679893917548189fd79018ff7d1cf4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_85275696b30247928a2209f2837ba926",
              "IPY_MODEL_7b667505084c4274ab80385aaa4e8c1d",
              "IPY_MODEL_e8db2ebf1708464b891e4665b219adbf"
            ],
            "layout": "IPY_MODEL_436726c28c884e68a660255160db7198"
          }
        },
        "85275696b30247928a2209f2837ba926": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_783b9e824b2e4e7282db68d3a49c6d27",
            "placeholder": "​",
            "style": "IPY_MODEL_06abf32f8e1d4c22a5ccd39e684838cc",
            "value": "train.jsonl: 100%"
          }
        },
        "7b667505084c4274ab80385aaa4e8c1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a665cf280c404342b5cf8ddaa3601f72",
            "max": 253475402,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8aecdd00aa34456ca4ea540fca48a2ce",
            "value": 253475402
          }
        },
        "e8db2ebf1708464b891e4665b219adbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_865c8fbde599442eb2a6aa7ee10f21fc",
            "placeholder": "​",
            "style": "IPY_MODEL_88c553ff3c924d2b92b43af3c1cc0b0c",
            "value": " 253M/253M [00:05&lt;00:00, 46.0MB/s]"
          }
        },
        "436726c28c884e68a660255160db7198": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "783b9e824b2e4e7282db68d3a49c6d27": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06abf32f8e1d4c22a5ccd39e684838cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a665cf280c404342b5cf8ddaa3601f72": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8aecdd00aa34456ca4ea540fca48a2ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "865c8fbde599442eb2a6aa7ee10f21fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88c553ff3c924d2b92b43af3c1cc0b0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5477310c81344d4a87318d8a3ad8f29d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b04d25cb4c1d4e688c190a98355952ac",
              "IPY_MODEL_e0d62bbe98e74c9c8f86df05effe177d",
              "IPY_MODEL_5faf179ef8274fb9a9082d349f603d51"
            ],
            "layout": "IPY_MODEL_680c541b023546258798d94985a9f80f"
          }
        },
        "b04d25cb4c1d4e688c190a98355952ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cfa4a48078ec46ff84f20eea0ece07d7",
            "placeholder": "​",
            "style": "IPY_MODEL_a44fa85d031a4e7fb34abf11aca5fdc0",
            "value": "Generating train split: 100%"
          }
        },
        "e0d62bbe98e74c9c8f86df05effe177d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_581ad864944248a0a116130df974d3b4",
            "max": 209760,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f71affce762c4ee98ca000920875f511",
            "value": 209760
          }
        },
        "5faf179ef8274fb9a9082d349f603d51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39a5a6f716f84c8fa35cda546383b596",
            "placeholder": "​",
            "style": "IPY_MODEL_ee42ab433eaf48cfb479578a77369df6",
            "value": " 209760/209760 [00:03&lt;00:00, 109105.95 examples/s]"
          }
        },
        "680c541b023546258798d94985a9f80f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cfa4a48078ec46ff84f20eea0ece07d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a44fa85d031a4e7fb34abf11aca5fdc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "581ad864944248a0a116130df974d3b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f71affce762c4ee98ca000920875f511": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "39a5a6f716f84c8fa35cda546383b596": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee42ab433eaf48cfb479578a77369df6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}